{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d775e2b1-4c66-4575-89e6-d47c9feb440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8765a3e-8050-41f8-b074-7f8f76641188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10430, 772)\n",
      "(1841, 772)\n",
      "(3068, 772)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/Annotation/joined/joined_train.csv', index_col='id')\n",
    "test_df = pd.read_csv('../data/Annotation/joined/joined_test.csv', index_col='id')\n",
    "full_train_df = train_df.copy()\n",
    "valid_df = train_df.sample(frac=0.15)\n",
    "train_df = train_df.drop(valid_df.index)\n",
    "print(train_df.shape)\n",
    "print(valid_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b35144b8-afc0-47eb-99a2-1c224f4cff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = train_df.shape[1] - 1\n",
    "num_labels = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "272dd0d6-9e9f-4717-904b-d854ea793cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(batch_size,\n",
    "                     full_train_features, full_train_targets,\n",
    "                     train_features, train_targets,\n",
    "                     valid_features, valid_targets,\n",
    "                     test_features, test_targets):\n",
    "    full_train_dataset = CustomDataset(full_train_features, full_train_targets)\n",
    "    full_train_data_loader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    train_dataset = CustomDataset(train_features, train_targets)\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataset = CustomDataset(valid_features, valid_targets)\n",
    "    valid_data_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataset = CustomDataset(test_features, test_targets)\n",
    "    test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return full_train_data_loader, train_data_loader, valid_data_loader, test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2333b224-4840-4039-8759-65ce1da4b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features.to(torch.float32)\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ft = self.features[index]\n",
    "        t = self.targets[index]\n",
    "        return ft, t\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23da2636-a866-41f7-8eb8-9be304dc33f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_X, full_train_y = torch.tensor(full_train_df.drop(columns=['label']).values), torch.tensor(full_train_df['label'].values)\n",
    "train_X, train_y = torch.tensor(train_df.drop(columns=['label']).values), torch.tensor(train_df['label'].values)\n",
    "valid_X, valid_y = torch.tensor(valid_df.drop(columns=['label']).values), torch.tensor(valid_df['label'].values)\n",
    "test_X, test_y = torch.tensor(test_df.drop(columns=['label']).values), torch.tensor(test_df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23077e99-9699-4b15-9a49-12787fdce22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "full_train_data_loader, train_data_loader, valid_data_loader, test_data_loader = get_data_loaders(batch_size,\n",
    "                                                                          full_train_X, full_train_y,\n",
    "                                                                          train_X, train_y,\n",
    "                                                                          valid_X, valid_y,\n",
    "                                                                          test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4154071e-0031-4ad4-8381-4123555d370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_features, hidden_layers, num_targets, activation='sigmoid', dropout=0.0):\n",
    "        \"\"\"\n",
    "        :param num_features: an integer denoting number of features, e.g., 784\n",
    "        :param hidden_layers: a tuple of integers denoting hidden layer counts e.g., (4, 5, 6,)\n",
    "        :param num_targets: an integer denoting number of targets, e.g., 10\n",
    "        :param activation: activation function to be used between neural network layers\n",
    "        :param dropout: probability of an element to be zeroed (for dropout layer)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if activation == 'sigmoid':\n",
    "            activation_function = nn.Sigmoid()\n",
    "        elif activation == 'relu':\n",
    "            activation_function = nn.ReLU()\n",
    "        else:\n",
    "            raise ValueError('activation must be one of \"sigmoid\" or \"relu\"')\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(num_features, hidden_layers[0]))\n",
    "        if (dropout > 0.0):\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        layers.append(activation_function)\n",
    "\n",
    "        for i in range(len(hidden_layers) - 1):\n",
    "            layers.append(nn.Linear(hidden_layers[i], hidden_layers[i + 1]))\n",
    "            if (dropout > 0.0):\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            layers.append(activation_function)\n",
    "        layers.append(nn.Linear(hidden_layers[-1], num_targets))\n",
    "\n",
    "        self.stack = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.stack(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98572832-de04-4365-9cf4-969b79625a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss, correct = 0, 0\n",
    "    num_batches = len(dataloader)\n",
    "    for X, y in dataloader:\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred = pred.argmax(1)\n",
    "        correct += (y_pred == y).type(torch.float).sum().item()\n",
    "\n",
    "    train_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    model_performance = {\n",
    "        'train_loss': train_loss,\n",
    "        'train_accuracy': 100 * correct\n",
    "    }\n",
    "\n",
    "    return model_performance\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, isValidation=True):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    y_pred_lst = []\n",
    "    y_lst = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            y_pred = pred.argmax(1)\n",
    "            y_pred_lst += y_pred.tolist()\n",
    "            y_lst += y.tolist()\n",
    "            correct += (y_pred == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    model_performance = {\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': 100 * correct\n",
    "    }\n",
    "\n",
    "    if not isValidation:\n",
    "        print(f\"Test Performance: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "        model_performance.update({'y_pred': y_pred_lst, 'y_true': y_lst})\n",
    "\n",
    "    return model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fa5982d-4c72-4365-acda-d0e55f9cf941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_neural_network(epochs, train_data_loader,\n",
    "                           neural_network, loss_fn, optimizer, valid_data_loader,\n",
    "                           test_data_loader):\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    train_acc = []\n",
    "    validation_acc = []\n",
    "\n",
    "\n",
    "    for t in range(epochs):\n",
    "        # training\n",
    "        model_performance_train = train_loop(train_data_loader, neural_network, loss_fn, optimizer)\n",
    "        train_loss.append(model_performance_train['train_loss'])\n",
    "        train_acc.append(model_performance_train['train_accuracy'])\n",
    "        if valid_data_loader is not None:\n",
    "            # validation\n",
    "            model_performance_valid = test_loop(valid_data_loader, neural_network, loss_fn)\n",
    "            validation_loss.append(model_performance_valid['test_loss'])\n",
    "            validation_acc.append(model_performance_valid['test_accuracy'])\n",
    "        if t % 10 == 0:\n",
    "            print(f'Iteration {t+1}')\n",
    "            print(f\"Training Performance: \\n Accuracy: {model_performance_train['train_accuracy']:>0.1f}%, \"\n",
    "                  f\"Avg loss: {model_performance_train['train_loss']:>8f}\")\n",
    "            if valid_data_loader is not None:\n",
    "                print(f\"Validation Performance: \\n Accuracy: {model_performance_valid['test_accuracy']:>0.1f}%, \"\n",
    "                  f\"Avg loss: {model_performance_valid['test_loss']:>8f} \\n\")\n",
    "\n",
    "    # test\n",
    "    model_performance_test = test_loop(test_data_loader, neural_network, loss_fn, isValidation=False)\n",
    "    # f1_score = get_f1_scores(model_performance_test['y_true'], model_performance_test['y_pred'])\n",
    "    # print(f\"\\nF1 Score: {f1_score:>0.2f}\")\n",
    "    # get_confusion_matrix(model_performance_test['y_true'], model_performance_test['y_pred'])\n",
    "    # plot_learning_curve(train_loss, validation_loss, train_acc, validation_acc, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a29c5097-972d-4f5d-a864-3a61d7538e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_expressions(dropout=0.00, weight_decay=0.0001, learning_rate=5e-4, epochs=500, valid=False):\n",
    "\n",
    "    # Instantiate the neural network\n",
    "    neural_network = NeuralNetwork(num_features, (8, 16, 8), num_labels)\n",
    "\n",
    "    # Initialize loss and optimizer, and train/test the neural network\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    adam_optimizer = torch.optim.Adam(neural_network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    if valid:\n",
    "        execute_neural_network(epochs, train_data_loader, neural_network, loss_fn, adam_optimizer, valid_data_loader,\n",
    "                           test_data_loader)\n",
    "    else:\n",
    "        execute_neural_network(epochs, full_train_data_loader, neural_network, loss_fn, adam_optimizer, None,\n",
    "                           test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0546fa4e-f06e-4625-b88c-e0a4c67be87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Training Performance: \n",
      " Accuracy: 38.9%, Avg loss: 1.678060\n",
      "Iteration 11\n",
      "Training Performance: \n",
      " Accuracy: 52.5%, Avg loss: 1.310235\n",
      "Iteration 21\n",
      "Training Performance: \n",
      " Accuracy: 53.0%, Avg loss: 1.283238\n",
      "Iteration 31\n",
      "Training Performance: \n",
      " Accuracy: 54.8%, Avg loss: 1.261495\n",
      "Iteration 41\n",
      "Training Performance: \n",
      " Accuracy: 55.5%, Avg loss: 1.238516\n",
      "Iteration 51\n",
      "Training Performance: \n",
      " Accuracy: 56.5%, Avg loss: 1.212470\n",
      "Iteration 61\n",
      "Training Performance: \n",
      " Accuracy: 58.2%, Avg loss: 1.162120\n",
      "Iteration 71\n",
      "Training Performance: \n",
      " Accuracy: 59.2%, Avg loss: 1.125081\n",
      "Iteration 81\n",
      "Training Performance: \n",
      " Accuracy: 60.4%, Avg loss: 1.098790\n",
      "Iteration 91\n",
      "Training Performance: \n",
      " Accuracy: 61.3%, Avg loss: 1.078981\n",
      "Iteration 101\n",
      "Training Performance: \n",
      " Accuracy: 61.9%, Avg loss: 1.069254\n",
      "Iteration 111\n",
      "Training Performance: \n",
      " Accuracy: 62.7%, Avg loss: 1.058170\n",
      "Iteration 121\n",
      "Training Performance: \n",
      " Accuracy: 63.4%, Avg loss: 1.040436\n",
      "Iteration 131\n",
      "Training Performance: \n",
      " Accuracy: 63.3%, Avg loss: 1.032011\n",
      "Iteration 141\n",
      "Training Performance: \n",
      " Accuracy: 64.1%, Avg loss: 1.015708\n",
      "Iteration 151\n",
      "Training Performance: \n",
      " Accuracy: 64.7%, Avg loss: 1.007840\n",
      "Iteration 161\n",
      "Training Performance: \n",
      " Accuracy: 64.9%, Avg loss: 1.001847\n",
      "Iteration 171\n",
      "Training Performance: \n",
      " Accuracy: 65.2%, Avg loss: 0.998706\n",
      "Iteration 181\n",
      "Training Performance: \n",
      " Accuracy: 65.5%, Avg loss: 0.990008\n",
      "Iteration 191\n",
      "Training Performance: \n",
      " Accuracy: 65.6%, Avg loss: 0.983409\n",
      "Iteration 201\n",
      "Training Performance: \n",
      " Accuracy: 65.6%, Avg loss: 0.984688\n",
      "Iteration 211\n",
      "Training Performance: \n",
      " Accuracy: 65.8%, Avg loss: 0.974796\n",
      "Iteration 221\n",
      "Training Performance: \n",
      " Accuracy: 66.2%, Avg loss: 0.969984\n",
      "Iteration 231\n",
      "Training Performance: \n",
      " Accuracy: 66.4%, Avg loss: 0.968450\n",
      "Iteration 241\n",
      "Training Performance: \n",
      " Accuracy: 66.4%, Avg loss: 0.961451\n",
      "Iteration 251\n",
      "Training Performance: \n",
      " Accuracy: 66.6%, Avg loss: 0.954440\n",
      "Iteration 261\n",
      "Training Performance: \n",
      " Accuracy: 67.0%, Avg loss: 0.946355\n",
      "Iteration 271\n",
      "Training Performance: \n",
      " Accuracy: 66.8%, Avg loss: 0.931827\n",
      "Iteration 281\n",
      "Training Performance: \n",
      " Accuracy: 67.2%, Avg loss: 0.921806\n",
      "Iteration 291\n",
      "Training Performance: \n",
      " Accuracy: 68.0%, Avg loss: 0.913108\n",
      "Iteration 301\n",
      "Training Performance: \n",
      " Accuracy: 67.7%, Avg loss: 0.907376\n",
      "Iteration 311\n",
      "Training Performance: \n",
      " Accuracy: 67.9%, Avg loss: 0.906474\n",
      "Iteration 321\n",
      "Training Performance: \n",
      " Accuracy: 68.1%, Avg loss: 0.903240\n",
      "Iteration 331\n",
      "Training Performance: \n",
      " Accuracy: 67.9%, Avg loss: 0.902506\n",
      "Iteration 341\n",
      "Training Performance: \n",
      " Accuracy: 68.2%, Avg loss: 0.902553\n",
      "Iteration 351\n",
      "Training Performance: \n",
      " Accuracy: 68.0%, Avg loss: 0.900216\n",
      "Iteration 361\n",
      "Training Performance: \n",
      " Accuracy: 68.1%, Avg loss: 0.894764\n",
      "Iteration 371\n",
      "Training Performance: \n",
      " Accuracy: 68.2%, Avg loss: 0.896837\n",
      "Iteration 381\n",
      "Training Performance: \n",
      " Accuracy: 68.2%, Avg loss: 0.895243\n",
      "Iteration 391\n",
      "Training Performance: \n",
      " Accuracy: 68.5%, Avg loss: 0.893667\n",
      "Iteration 401\n",
      "Training Performance: \n",
      " Accuracy: 68.5%, Avg loss: 0.893483\n",
      "Iteration 411\n",
      "Training Performance: \n",
      " Accuracy: 68.3%, Avg loss: 0.891607\n",
      "Iteration 421\n",
      "Training Performance: \n",
      " Accuracy: 68.5%, Avg loss: 0.889967\n",
      "Iteration 431\n",
      "Training Performance: \n",
      " Accuracy: 68.5%, Avg loss: 0.888399\n",
      "Iteration 441\n",
      "Training Performance: \n",
      " Accuracy: 68.4%, Avg loss: 0.888095\n",
      "Iteration 451\n",
      "Training Performance: \n",
      " Accuracy: 68.6%, Avg loss: 0.886900\n",
      "Iteration 461\n",
      "Training Performance: \n",
      " Accuracy: 68.7%, Avg loss: 0.886199\n",
      "Iteration 471\n",
      "Training Performance: \n",
      " Accuracy: 68.6%, Avg loss: 0.887504\n",
      "Iteration 481\n",
      "Training Performance: \n",
      " Accuracy: 68.6%, Avg loss: 0.889130\n",
      "Iteration 491\n",
      "Training Performance: \n",
      " Accuracy: 69.0%, Avg loss: 0.885419\n",
      "Test Performance: \n",
      " Accuracy: 68.4%, Avg loss: 0.921079 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "classify_expressions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
